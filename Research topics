Research


What optimizer to use? Adam?  SGD?

David Giordano (student) says try Adam first then SGD.  
'7 tips to choose the best optimizer' 2020
https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e

Zhou et al (good paper) says SGD optimizes better than Adam 
'Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning' 2020
https://proceedings.neurips.cc/paper/2020/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf

Jason Brownlee (PhD) says Adam is a good choice
Gentle Introduction to the Adam Optimization Algorithm for Deep Learning' 2017
https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/


Choice of loss op?


Number of epochs?  Ensure that it converges


Learning_rate? Ensure that it converges (not too small) and doesn't oscillate (not too large).



Implement dropConnect or dropOut
https://stackoverflow.com/questions/44355229/can-i-use-tf-nn-dropout-to-implement-dropconnect
Dropconnect doesn't look like it helps with 1x [2048] neurons.


