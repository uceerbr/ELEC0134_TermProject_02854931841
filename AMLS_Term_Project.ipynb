{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc265e3",
   "metadata": {},
   "source": [
    "#  Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea0b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "(3000, 64, 64)\n",
      "(200, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Term project for AMLS_ELEC0134\n",
    "#\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mplp\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "'''imagePath = './dataset/image/IMAGE_0574.jpg'\n",
    "imgFile = cv2.imread(imagePath,0)\n",
    "#print(imgFile.shape)\n",
    "minVal=numpy.amin(imgFile)\n",
    "#print('min= ',minVal)\n",
    "maxVal= numpy.amax(imgFile)\n",
    "#print('max= ',maxVal)\n",
    "#mplp.imshow(imgFile)\n",
    "#mplp.show()'''\n",
    "\n",
    "#\n",
    "# load the images\n",
    "target_size=[64,64]\n",
    "\n",
    "training_images_dir='./trainingDataset/image'\n",
    "training_images_paths = [os.path.join(training_images_dir,l) for l in os.listdir(training_images_dir)]\n",
    "training_images = []\n",
    "#print(training_images_paths)\n",
    "for img_path in training_images_paths:\n",
    "    filename=img_path.split('\\\\')[-1]\n",
    "    #print('filename=',filename)\n",
    "    #target_size=None\n",
    "    img = image.img_to_array(image.load_img(img_path,target_size=target_size, interpolation='bicubic'))\n",
    "    #img = image.img_to_array(image.load_img(img_path,target_size=[64,64], interpolation='bicubic'))\n",
    "    imgBW = img[:,:,0]\n",
    "    #print(img.shape)\n",
    "    #print(imgBW.shape)\n",
    "    '''if (filename=='IMAGE_0000.jpg'):\n",
    "        mplp.imshow(img/255)\n",
    "        mplp.show()\n",
    "        mplp.imshow(imgBW/255)\n",
    "        mplp.show()'''\n",
    "    training_images.append(imgBW)\n",
    "trainingImages=numpy.array(training_images)\n",
    "print(trainingImages.shape)\n",
    "\n",
    "test_images_dir='./testDataset/image'\n",
    "test_images_paths = [os.path.join(test_images_dir,l) for l in os.listdir(test_images_dir)]\n",
    "test_images = []\n",
    "for img_path in test_images_paths:\n",
    "    filename=img_path.split('\\\\')[-1]\n",
    "    #print('filename=',filename)\n",
    "    #target_size=None\n",
    "    img = image.img_to_array(image.load_img(img_path,target_size=target_size, interpolation='bicubic'))\n",
    "    #img = image.img_to_array(image.load_img(img_path,target_size=[64,64], interpolation='bicubic'))\n",
    "    imgBW = img[:,:,0]\n",
    "    #print(img.shape)\n",
    "    #print(imgBW.shape)\n",
    "    '''if (filename=='IMAGE_0000.jpg'):\n",
    "        mplp.imshow(img/255)\n",
    "        mplp.show()\n",
    "        mplp.imshow(imgBW/255)\n",
    "        mplp.show()'''\n",
    "    test_images.append(imgBW)\n",
    "testImages=numpy.array(test_images)\n",
    "print(testImages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a45ac2",
   "metadata": {},
   "source": [
    "# Import tumor labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b49357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingTumorNumbers is  int32\n",
      "trainingTumorNumbers is  (3000,)\n",
      "(3000, 2)\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "testTumorNumbers is  int32\n",
      "testTumorNumbers is  (200,)\n",
      "(200, 2)\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "trainingTumorLabels= pandas.read_csv('./trainingDataset/label.csv')\n",
    "#print(trainingTumorLabels.shape)\n",
    "#print(trainingTumorLabels)\n",
    "trainingTumorLabels=trainingTumorLabels['label']\n",
    "#print(trainingTumorLabels.shape)\n",
    "#print(trainingTumorLabels)\n",
    "'''tumorNumbers = [float(0.0) if label=='no_tumor' else float(1.0) for label in tumorLabels]\n",
    "for (i,label) in enumerate(tumorLabels):\n",
    "    if label=='glioma_tumor':\n",
    "        tumorNumbers[i]=1.0\n",
    "    else: \n",
    "        if label == 'meningioma_tumor':\n",
    "            tumorNumbers[i]=2.0\n",
    "        else:\n",
    "            if label == 'pituitary_tumor':\n",
    "                tumorNumbers[i]=3.0\n",
    "            else:\n",
    "                tumorNumbers[i]=0.0'''\n",
    "trainingTumorNumbers = [0 if label=='no_tumor' else 1 for label in trainingTumorLabels]\n",
    "trainingTumorNumbers = numpy.array(trainingTumorNumbers)\n",
    "print('trainingTumorNumbers is ',trainingTumorNumbers.dtype)\n",
    "print('trainingTumorNumbers is ',trainingTumorNumbers.shape)\n",
    "#print(trainingTumorNumbers)\n",
    "trainingTumorClasses = numpy.array([trainingTumorNumbers, 1-trainingTumorNumbers]).T\n",
    "print(trainingTumorClasses.shape)\n",
    "print(trainingTumorClasses)\n",
    "\n",
    "\n",
    "testTumorLabels= pandas.read_csv('./testDataset/label.csv')\n",
    "#print(testTumorLabels.shape)\n",
    "#print(testTumorLabels)\n",
    "testTumorLabels=testTumorLabels['label']\n",
    "#print(testTumorLabels.shape)\n",
    "#print(testTumorLabels)\n",
    "'''tumorNumbers = [float(0.0) if label=='no_tumor' else float(1.0) for label in tumorLabels]\n",
    "for (i,label) in enumerate(tumorLabels):\n",
    "    if label=='glioma_tumor':\n",
    "        tumorNumbers[i]=1.0\n",
    "    else: \n",
    "        if label == 'meningioma_tumor':\n",
    "            tumorNumbers[i]=2.0\n",
    "        else:\n",
    "            if label == 'pituitary_tumor':\n",
    "                tumorNumbers[i]=3.0\n",
    "            else:\n",
    "                tumorNumbers[i]=0.0'''\n",
    "testTumorNumbers = [0 if label=='no_tumor' else 1 for label in testTumorLabels]\n",
    "testTumorNumbers = numpy.array(testTumorNumbers)\n",
    "print('testTumorNumbers is ',testTumorNumbers.dtype)\n",
    "print('testTumorNumbers is ',testTumorNumbers.shape)\n",
    "#print(testTrainingTumorNumbers)\n",
    "testTumorClasses = numpy.array([testTumorNumbers, 1-testTumorNumbers]).T\n",
    "print(testTumorClasses.shape)\n",
    "print(testTumorClasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ba28e",
   "metadata": {},
   "source": [
    "# Define an MLP with 1-4 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d56ec8",
   "metadata": {},
   "source": [
    "# Input the number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4447872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def allocate_weights_and_biases(neurons_hidden):\n",
    "    \n",
    "    # define number of hidden layers ..\n",
    "    num_layers = len(neurons_hidden)     \n",
    "\n",
    "    # inputs placeholders\n",
    "    X = tf.placeholder(\"float\", [None, 64,64]) # images are 64x64\n",
    "    Y = tf.placeholder(\"float\", [None, 2])  # 2 output classes\n",
    "    \n",
    "    # flatten image features into one vector (i.e. reshape image feature matrix into a vector)\n",
    "    images_flat = tf.layers.flatten(X)  \n",
    "    #images_flat = tf.keras.layers.Flatten(X) # this doesn't work\n",
    "    \n",
    "    # weights and biases are initialized from a normal distribution with a specified standard devation stddev\n",
    "    stddev = 0.01\n",
    "    \n",
    "    # define placeholders for weights and biases in the graph\n",
    "    if num_layers==0:\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([64*64, 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "    if num_layers==1:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "        \n",
    "    if num_layers==2:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "              \n",
    "    if num_layers==3:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'hidden_layer3': tf.Variable(tf.random_normal([neurons_hidden[1], neurons_hidden[2]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'bias_layer3': tf.Variable(tf.random_normal([neurons_hidden[2]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "           \n",
    "    if num_layers==4:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'hidden_layer3': tf.Variable(tf.random_normal([neurons_hidden[1], neurons_hidden[2]], stddev=stddev)),\n",
    "            'hidden_layer4': tf.Variable(tf.random_normal([neurons_hidden[2], neurons_hidden[3]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'bias_layer3': tf.Variable(tf.random_normal([neurons_hidden[2]], stddev=stddev)),\n",
    "            'bias_layer4': tf.Variable(tf.random_normal([neurons_hidden[3]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "    \n",
    "    return weights, biases, X, Y, images_flat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3381dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(neurons_hidden):\n",
    "        \n",
    "    num_layers = len(neurons_hidden) \n",
    "    weights, biases, X, Y, images_flat = allocate_weights_and_biases(neurons_hidden=neurons_hidden)\n",
    "        \n",
    "    # Hidden fully connected layer 1\n",
    "    if (num_layers >=1):\n",
    "        layer_1 = tf.add(tf.matmul(images_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
    "        layer_1 = tf.sigmoid(layer_1)\n",
    "        layer_final = layer_1\n",
    "\n",
    "    # Hidden fully connected layer 2\n",
    "    if (num_layers>=2):\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
    "        layer_2 = tf.sigmoid(layer_2)\n",
    "        layer_final = layer_2\n",
    "\n",
    "    # Hidden fully connected layer 3\n",
    "    if (num_layers>=3):\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['hidden_layer3']), biases['bias_layer3'])\n",
    "        layer_3 = tf.sigmoid(layer_3)\n",
    "        layer_final = layer_3\n",
    "\n",
    "    # Hidden fully connected layer 4\n",
    "    if (num_layers>=4):\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['hidden_layer4']), biases['bias_layer4'])\n",
    "        layer_4 = tf.sigmoid(layer_4)\n",
    "        layer_final = layer_4                     \n",
    "    \n",
    "    # Output fully connected layer\n",
    "    out_layer = tf.matmul(layer_final, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff85ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictionClasses, actualClasses,labels):\n",
    "   #print(testClasses.T)\n",
    "   #print(predictions_onehot.T)\n",
    "   print('\\nCONFUSION MATRIX')\n",
    "   print('Actual values in X')\n",
    "   print('Predicted values in Y')\n",
    "   confusionMatrix = confusion_matrix(predictionClasses.T[0],actualClasses.T[0])\n",
    "   print(confusionMatrix,'\\n')\n",
    " \n",
    "   inputNumbers = numpy.sum(confusionMatrix, axis=0)\n",
    "   print(inputNumbers[0],' cases of ',labels[0],': ',round(100*confusionMatrix[0,0]/inputNumbers[0],2),'% chance of correct prediction')\n",
    "   print(inputNumbers[1],' cases of ',labels[1],': ',round(100*confusionMatrix[1,1]/inputNumbers[1],2),'% chance of correct prediction\\n') \n",
    "\n",
    "   outputNumbers = numpy.sum(confusionMatrix, axis=1)\n",
    "   print(outputNumbers[0],' predictions of ',labels[0],' are correct ',round(100*confusionMatrix[0,0]/outputNumbers[0],2),'% of the time')\n",
    "   print(outputNumbers[1],' predictions of ',labels[1],' are correct ',round(100*confusionMatrix[1,1]/outputNumbers[1],2),'% of the time')\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f03e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_run_mlp(neurons_hidden,learning_rate, training_epochs):\n",
    "  # display training accuracy every ..\n",
    "  display_accuracy_step = 10\n",
    "\n",
    "  logits, X, Y = multilayer_perceptron(neurons_hidden=neurons_hidden)\n",
    "\n",
    "  # define loss and optimizer\n",
    "  loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "  # define training graph operation\n",
    "  train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "  # graph operation to initialize all variables\n",
    "  init_op = tf.global_variables_initializer()\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    num_sessions = 1\n",
    "    accuracy_eval = numpy.zeros(num_sessions)\n",
    "    \n",
    "    for session in range(num_sessions):\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init_op)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # complete code below\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            #_, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
    "                                                               #Y: trainingClasses})\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: trainingImages,\n",
    "                                                               Y: trainingTumorClasses})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            #print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                #print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: trainingClasses})))\n",
    "\n",
    "        print('\\nSession ',session+1,\" Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            \n",
    "        # run test accuracy operation ..\n",
    "        accuracy_eval[session] = accuracy.eval({X: testImages, Y: testTumorClasses})\n",
    "        print('Session ',session+1,\" Test Accuracy:\", round(100*accuracy_eval[session],2),'%')\n",
    "        \n",
    "        predictions = pred.eval(feed_dict = {X:testImages})\n",
    "        #print(predictions.T)\n",
    "        predictions_onehot = predictions.copy()\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            #print(i,'   ',prediction)\n",
    "            if prediction[0] > prediction[1]:\n",
    "                predictions_onehot[i] = [1,0]\n",
    "            else:\n",
    "                predictions_onehot[i] = [0,1]\n",
    "        #print(predictions_onehot.T)\n",
    "        print_confusion_matrix(predictions_onehot,testTumorClasses, labels=['no tumor','a tumor'])\n",
    "        \n",
    "    print('\\nMean Test Accuracy = ',round(100*numpy.mean(accuracy_eval),2),'%\\n\\n')\n",
    "    return(predictions_onehot, testTumorClasses)\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b28716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 81.5 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[  0   0]\n",
      " [ 37 163]] \n",
      "\n",
      "37  cases of  no tumor :  0.0 % chance of correct prediction\n",
      "163  cases of  a tumor :  100.0 % chance of correct prediction\n",
      "\n",
      "0  predictions of  no tumor  are correct  nan % of the time\n",
      "200  predictions of  a tumor  are correct  81.5 % of the time\n",
      "\n",
      "Mean Test Accuracy =  81.5 %\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    " #dummy command to get rid of deprecation warning.\n",
    "predictions_onehot, testTumorClasses = initialize_and_run_mlp([1], learning_rate=0.01, training_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e330edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  hidden layers\n",
      "[2048, 2048, 2048, 2048]  neurons\n",
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 92.5 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[ 24   2]\n",
      " [ 13 161]] \n",
      "\n",
      "37  cases of  no tumor :  64.86 % chance of correct prediction\n",
      "163  cases of  a tumor :  98.77 % chance of correct prediction\n",
      "\n",
      "26  predictions of  no tumor  are correct  92.31 % of the time\n",
      "174  predictions of  a tumor  are correct  92.53 % of the time\n",
      "\n",
      "Mean Test Accuracy =  92.5 %\n",
      "\n",
      "\n",
      "3  hidden layers\n",
      "[2048, 2048, 2048]  neurons\n",
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 92.5 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[ 23   1]\n",
      " [ 14 162]] \n",
      "\n",
      "37  cases of  no tumor :  62.16 % chance of correct prediction\n",
      "163  cases of  a tumor :  99.39 % chance of correct prediction\n",
      "\n",
      "24  predictions of  no tumor  are correct  95.83 % of the time\n",
      "176  predictions of  a tumor  are correct  92.05 % of the time\n",
      "\n",
      "Mean Test Accuracy =  92.5 %\n",
      "\n",
      "\n",
      "2  hidden layers\n",
      "[2048, 2048]  neurons\n",
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 92.0 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[ 23   2]\n",
      " [ 14 161]] \n",
      "\n",
      "37  cases of  no tumor :  62.16 % chance of correct prediction\n",
      "163  cases of  a tumor :  98.77 % chance of correct prediction\n",
      "\n",
      "25  predictions of  no tumor  are correct  92.0 % of the time\n",
      "175  predictions of  a tumor  are correct  92.0 % of the time\n",
      "\n",
      "Mean Test Accuracy =  92.0 %\n",
      "\n",
      "\n",
      "1  hidden layers\n",
      "[2048]  neurons\n",
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 91.5 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[ 22   2]\n",
      " [ 15 161]] \n",
      "\n",
      "37  cases of  no tumor :  59.46 % chance of correct prediction\n",
      "163  cases of  a tumor :  98.77 % chance of correct prediction\n",
      "\n",
      "24  predictions of  no tumor  are correct  91.67 % of the time\n",
      "176  predictions of  a tumor  are correct  91.48 % of the time\n",
      "\n",
      "Mean Test Accuracy =  91.5 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "neurons_hidden = [2048,2048,2048,2048]\n",
    "num_layers = len(neurons_hidden) # default\n",
    "num_layers = 1  # override\n",
    "for num_layers in range(4,0,-1):\n",
    "    neurons_hidden = neurons_hidden[:num_layers]\n",
    "    print(num_layers,' hidden layers')\n",
    "    print(neurons_hidden,' neurons')\n",
    "    predictions_onehot, testTumorClasses = initialize_and_run_mlp(neurons_hidden=neurons_hidden, learning_rate=learning_rate, training_epochs=training_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293e423",
   "metadata": {},
   "source": [
    "# MLP with SGD (instead of Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33431a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_run_mlp_with_sgd(neurons_hidden,learning_rate, training_epochs):\n",
    "  # display training accuracy every ..\n",
    "  display_accuracy_step = 10\n",
    "\n",
    "  logits, X, Y = multilayer_perceptron(neurons_hidden=neurons_hidden)\n",
    "\n",
    "  # define loss and optimizer\n",
    "  loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "  #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "  #var = tf.Variable(1.0)\n",
    "  #loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
    "  #step_count = opt.minimize(loss, [var]).numpy()\n",
    "  # Step is `- learning_rate * grad`\n",
    "  #var.numpy()\n",
    "\n",
    "  # define training graph operation\n",
    "  #train_op = optimizer.minimoze(loss_op)\n",
    "  train_op = optimizer.minimize(loss_op, var_list = None)\n",
    "  #train_op = optimizer.minimize(loss_op, var_list = logits)\n",
    "  #train_op = optimizer.minimize(loss_op, var_list = weights)\n",
    "  \n",
    "  \n",
    "  # graph operation to initialize all variables\n",
    "  init_op = tf.global_variables_initializer()\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    num_sessions = 1\n",
    "    accuracy_eval = numpy.zeros(num_sessions)\n",
    "    \n",
    "    for session in range(num_sessions):\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init_op)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # complete code below\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: trainingImages,\n",
    "                                                               Y: trainingTumorClasses})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            #print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                #print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: trainingClasses})))\n",
    "\n",
    "        print('\\nSession ',session+1,\" Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            \n",
    "        # run test accuracy operation ..\n",
    "        accuracy_eval[session] = accuracy.eval({X: testImages, Y: testTumorClasses})\n",
    "        print('Session ',session+1,\" Test Accuracy:\", round(100*accuracy_eval[session],2),'%')\n",
    "        \n",
    "        predictions = pred.eval(feed_dict = {X:testImages})\n",
    "        #print(predictions.T)\n",
    "        predictions_onehot = predictions.copy()\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            #print(i,'   ',prediction)\n",
    "            if prediction[0] > prediction[1]:\n",
    "                predictions_onehot[i] = [1,0]\n",
    "            else:\n",
    "                predictions_onehot[i] = [0,1]\n",
    "        #print(predictions_onehot.T)\n",
    "        print_confusion_matrix(predictions_onehot,testTumorClasses, labels=['no tumor','a tumor'])\n",
    "        \n",
    "    print('\\nMean Test Accuracy = ',round(100*numpy.mean(accuracy_eval),2),'%\\n\\n')\n",
    "    return(predictions_onehot, testTumorClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76389bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  hidden layers\n",
      "[2048]  neurons\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed in object of type <class 'NoneType'>, not tf.Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-630545c97c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' hidden layers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneurons_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' neurons'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpredictions_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestTumorClasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_and_run_mlp_with_sgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneurons_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneurons_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-24aa9d2940db>\u001b[0m in \u001b[0;36minitialize_and_run_mlp_with_sgd\u001b[1;34m(neurons_hidden, learning_rate, training_epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[1;31m# define training graph operation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[1;31m#train_op = optimizer.minimoze(loss_op)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m   \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m   \u001b[1;31m#train_op = optimizer.minimize(loss_op, var_list = logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[1;32m--> 316\u001b[1;33m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mwatch\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         raise ValueError(\"Passed in object of type {}, not tf.Tensor\".format(\n\u001b[1;32m--> 862\u001b[1;33m             type(t)))\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         logging.log_first_n(\n",
      "\u001b[1;31mValueError\u001b[0m: Passed in object of type <class 'NoneType'>, not tf.Tensor"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "neurons_hidden = [2048,2048,2048,2048]\n",
    "num_layers = len(neurons_hidden) # default\n",
    "num_layers = 1  # override\n",
    "\n",
    "neurons_hidden = neurons_hidden[:num_layers]    \n",
    "print(num_layers,' hidden layers')\n",
    "print(neurons_hidden,' neurons')\n",
    "predictions_onehot, testTumorClasses = initialize_and_run_mlp_with_sgd(neurons_hidden=neurons_hidden, learning_rate=learning_rate, training_epochs=training_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6fe7e",
   "metadata": {},
   "source": [
    "# MLP with dropconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e14f8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron_with_dropconnect(neurons_hidden):\n",
    "        \n",
    "    num_layers = len(neurons_hidden) \n",
    "    weights, biases, X, Y, images_flat = allocate_weights_and_biases(neurons_hidden=neurons_hidden)\n",
    "    \n",
    "    keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "    keep_prob = 0.5\n",
    "    rate=1 - keep_prob\n",
    "        \n",
    "    # Hidden fully connected layer 1\n",
    "    if (num_layers >=1):\n",
    "        dropConnect = tf.nn.dropout(weights['hidden_layer1'], rate) * rate\n",
    "        layer_1 = tf.add(tf.matmul(images_flat, dropConnect), biases['bias_layer1'])\n",
    "        layer_1 = tf.sigmoid(layer_1)\n",
    "        layer_final = layer_1\n",
    "\n",
    "    # Hidden fully connected layer 2\n",
    "    if (num_layers>=2):\n",
    "        dropConnect = tf.nn.dropout(weights['hidden_layer2'], rate) * rate\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, dropConnect), biases['bias_layer2'])\n",
    "        layer_2 = tf.sigmoid(layer_2)\n",
    "        layer_final = layer_2\n",
    "\n",
    "    # Hidden fully connected layer 3\n",
    "    if (num_layers>=3):\n",
    "        dropConnect = tf.nn.dropout(weights['hidden_layer3'], rate) * rate\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, dropConnect), biases['bias_layer3'])\n",
    "        layer_3 = tf.sigmoid(layer_3)\n",
    "        layer_final = layer_3\n",
    "\n",
    "    # Hidden fully connected layer 4\n",
    "    if (num_layers>=4):\n",
    "        dropConnect = tf.nn.dropout(weights['hidden_layer4'], rate) * rate\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, dropConnect), biases['bias_layer4'])\n",
    "        layer_4 = tf.sigmoid(layer_4)\n",
    "        layer_final = layer_4                     \n",
    "    \n",
    "    # Output fully connected layer\n",
    "    dropConnect = tf.nn.dropout(weights['out'], rate) * rate\n",
    "    out_layer = tf.matmul(layer_final, dropConnect) + biases['out']\n",
    "\n",
    "    return out_layer, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e1c41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_run_mlp_with_dropconnect(neurons_hidden,learning_rate, training_epochs):\n",
    "  # display training accuracy every ..\n",
    "  display_accuracy_step = 10\n",
    "\n",
    "  logits, X, Y = multilayer_perceptron_with_dropconnect(neurons_hidden=neurons_hidden)\n",
    "\n",
    "  # define loss and optimizer\n",
    "  loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "  # define training graph operation\n",
    "  train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "  # graph operation to initialize all variables\n",
    "  init_op = tf.global_variables_initializer()\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    num_sessions = 1\n",
    "    accuracy_eval = numpy.zeros(num_sessions)\n",
    "    \n",
    "    for session in range(num_sessions):\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init_op)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # complete code below\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            #_, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
    "                                                               #Y: trainingClasses})\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: trainingImages,\n",
    "                                                               Y: trainingTumorClasses})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            #print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                #print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: trainingClasses})))\n",
    "\n",
    "        print('\\nSession ',session+1,\" Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            \n",
    "        # run test accuracy operation ..\n",
    "        accuracy_eval[session] = accuracy.eval({X: testImages, Y: testTumorClasses})\n",
    "        print('Session ',session+1,\" Test Accuracy:\", round(100*accuracy_eval[session],2),'%')\n",
    "        \n",
    "        predictions = pred.eval(feed_dict = {X:testImages})\n",
    "        #print(predictions.T)\n",
    "        predictions_onehot = predictions.copy()\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            #print(i,'   ',prediction)\n",
    "            if prediction[0] > prediction[1]:\n",
    "                predictions_onehot[i] = [1,0]\n",
    "            else:\n",
    "                predictions_onehot[i] = [0,1]\n",
    "        #print(predictions_onehot.T)\n",
    "        print_confusion_matrix(predictions_onehot,testTumorClasses, labels=['no tumor','a tumor'])\n",
    "        \n",
    "    print('\\nMean Test Accuracy = ',round(100*numpy.mean(accuracy_eval),2),'%\\n\\n')\n",
    "    return(predictions_onehot, testTumorClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16e1ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  hidden layers\n",
      "[2048]  neurons\n",
      "\n",
      "Session  1  Optimization Finished!\n",
      "Session  1  Test Accuracy: 81.5 %\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[  0   0]\n",
      " [ 37 163]] \n",
      "\n",
      "37  cases of  no tumor :  0.0 % chance of correct prediction\n",
      "163  cases of  a tumor :  100.0 % chance of correct prediction\n",
      "\n",
      "0  predictions of  no tumor  are correct  nan % of the time\n",
      "200  predictions of  a tumor  are correct  81.5 % of the time\n",
      "\n",
      "Mean Test Accuracy =  81.5 %\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "neurons_hidden = [2048,2048,2048,2048]\n",
    "num_layers = len(neurons_hidden) # default\n",
    "num_layers = 1  # override\n",
    "\n",
    "neurons_hidden = neurons_hidden[:num_layers]    \n",
    "print(num_layers,' hidden layers')\n",
    "print(neurons_hidden,' neurons')\n",
    "predictions_onehot, testTumorClasses = initialize_and_run_mlp_with_dropconnect(neurons_hidden=neurons_hidden, learning_rate=learning_rate, training_epochs=training_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00916997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
