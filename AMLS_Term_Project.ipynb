{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc265e3",
   "metadata": {},
   "source": [
    "#  Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea0b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "(3000, 64, 64)\n",
      "(200, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Term project for AMLS_ELEC0134\n",
    "#\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mplp\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "'''imagePath = './dataset/image/IMAGE_0574.jpg'\n",
    "imgFile = cv2.imread(imagePath,0)\n",
    "#print(imgFile.shape)\n",
    "minVal=numpy.amin(imgFile)\n",
    "#print('min= ',minVal)\n",
    "maxVal= numpy.amax(imgFile)\n",
    "#print('max= ',maxVal)\n",
    "#mplp.imshow(imgFile)\n",
    "#mplp.show()'''\n",
    "\n",
    "#\n",
    "# load the images\n",
    "target_size=[64,64]\n",
    "\n",
    "training_images_dir='./trainingDataset/image'\n",
    "training_images_paths = [os.path.join(training_images_dir,l) for l in os.listdir(training_images_dir)]\n",
    "training_images = []\n",
    "#print(training_images_paths)\n",
    "for img_path in training_images_paths:\n",
    "    filename=img_path.split('\\\\')[-1]\n",
    "    #print('filename=',filename)\n",
    "    #target_size=None\n",
    "    img = image.img_to_array(image.load_img(img_path,target_size=target_size, interpolation='bicubic'))\n",
    "    #img = image.img_to_array(image.load_img(img_path,target_size=[64,64], interpolation='bicubic'))\n",
    "    imgBW = img[:,:,0]\n",
    "    #print(img.shape)\n",
    "    #print(imgBW.shape)\n",
    "    '''if (filename=='IMAGE_0000.jpg'):\n",
    "        mplp.imshow(img/255)\n",
    "        mplp.show()\n",
    "        mplp.imshow(imgBW/255)\n",
    "        mplp.show()'''\n",
    "    training_images.append(imgBW)\n",
    "trainingImages=numpy.array(training_images)\n",
    "print(trainingImages.shape)\n",
    "\n",
    "test_images_dir='./testDataset/image'\n",
    "test_images_paths = [os.path.join(test_images_dir,l) for l in os.listdir(test_images_dir)]\n",
    "test_images = []\n",
    "for img_path in test_images_paths:\n",
    "    filename=img_path.split('\\\\')[-1]\n",
    "    #print('filename=',filename)\n",
    "    #target_size=None\n",
    "    img = image.img_to_array(image.load_img(img_path,target_size=target_size, interpolation='bicubic'))\n",
    "    #img = image.img_to_array(image.load_img(img_path,target_size=[64,64], interpolation='bicubic'))\n",
    "    imgBW = img[:,:,0]\n",
    "    #print(img.shape)\n",
    "    #print(imgBW.shape)\n",
    "    '''if (filename=='IMAGE_0000.jpg'):\n",
    "        mplp.imshow(img/255)\n",
    "        mplp.show()\n",
    "        mplp.imshow(imgBW/255)\n",
    "        mplp.show()'''\n",
    "    test_images.append(imgBW)\n",
    "testImages=numpy.array(test_images)\n",
    "print(testImages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a45ac2",
   "metadata": {},
   "source": [
    "# Import tumor labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b49357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingTumorNumbers is  int32\n",
      "trainingTumorNumbers is  (3000,)\n",
      "(3000, 2)\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "testTumorNumbers is  int32\n",
      "testTumorNumbers is  (200,)\n",
      "(200, 2)\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "trainingTumorLabels= pandas.read_csv('./trainingDataset/label.csv')\n",
    "#print(trainingTumorLabels.shape)\n",
    "#print(trainingTumorLabels)\n",
    "trainingTumorLabels=trainingTumorLabels['label']\n",
    "#print(trainingTumorLabels.shape)\n",
    "#print(trainingTumorLabels)\n",
    "'''tumorNumbers = [float(0.0) if label=='no_tumor' else float(1.0) for label in tumorLabels]\n",
    "for (i,label) in enumerate(tumorLabels):\n",
    "    if label=='glioma_tumor':\n",
    "        tumorNumbers[i]=1.0\n",
    "    else: \n",
    "        if label == 'meningioma_tumor':\n",
    "            tumorNumbers[i]=2.0\n",
    "        else:\n",
    "            if label == 'pituitary_tumor':\n",
    "                tumorNumbers[i]=3.0\n",
    "            else:\n",
    "                tumorNumbers[i]=0.0'''\n",
    "trainingTumorNumbers = [0 if label=='no_tumor' else 1 for label in trainingTumorLabels]\n",
    "trainingTumorNumbers = numpy.array(trainingTumorNumbers)\n",
    "print('trainingTumorNumbers is ',trainingTumorNumbers.dtype)\n",
    "print('trainingTumorNumbers is ',trainingTumorNumbers.shape)\n",
    "#print(trainingTumorNumbers)\n",
    "trainingTumorClasses = numpy.array([trainingTumorNumbers, 1-trainingTumorNumbers]).T\n",
    "print(trainingTumorClasses.shape)\n",
    "print(trainingTumorClasses)\n",
    "\n",
    "\n",
    "testTumorLabels= pandas.read_csv('./testDataset/label.csv')\n",
    "#print(testTumorLabels.shape)\n",
    "#print(testTumorLabels)\n",
    "testTumorLabels=testTumorLabels['label']\n",
    "#print(testTumorLabels.shape)\n",
    "#print(testTumorLabels)\n",
    "'''tumorNumbers = [float(0.0) if label=='no_tumor' else float(1.0) for label in tumorLabels]\n",
    "for (i,label) in enumerate(tumorLabels):\n",
    "    if label=='glioma_tumor':\n",
    "        tumorNumbers[i]=1.0\n",
    "    else: \n",
    "        if label == 'meningioma_tumor':\n",
    "            tumorNumbers[i]=2.0\n",
    "        else:\n",
    "            if label == 'pituitary_tumor':\n",
    "                tumorNumbers[i]=3.0\n",
    "            else:\n",
    "                tumorNumbers[i]=0.0'''\n",
    "testTumorNumbers = [0 if label=='no_tumor' else 1 for label in testTumorLabels]\n",
    "testTumorNumbers = numpy.array(testTumorNumbers)\n",
    "print('testTumorNumbers is ',testTumorNumbers.dtype)\n",
    "print('testTumorNumbers is ',testTumorNumbers.shape)\n",
    "#print(testTrainingTumorNumbers)\n",
    "testTumorClasses = numpy.array([testTumorNumbers, 1-testTumorNumbers]).T\n",
    "print(testTumorClasses.shape)\n",
    "print(testTumorClasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8530651",
   "metadata": {},
   "source": [
    "# Define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f8f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_weights_and_biases_old():\n",
    "\n",
    "    # define number of hidden layers ..\n",
    "    n_hidden_1 = 2048  # 1st layer number of neurons\n",
    "    n_hidden_2 = 2048  # 2nd layer number of neurons\n",
    "\n",
    "    # inputs placeholders\n",
    "    #X = tf.placeholder(\"float\", [None, 68, 2])\n",
    "    X = tf.placeholder(\"float\", [None, 64, 64])\n",
    "    Y = tf.placeholder(\"float\", [None, 2])  # 2 output classes\n",
    "    #Y = tf.placeholder(\"float\", [None, 4])  # 4 output classes\n",
    "    \n",
    "    # flatten image features into one vector (i.e. reshape image feature matrix into a vector)\n",
    "    images_flat = tf.layers.flatten(X)  # This generates a warning\n",
    "    #images_flat = tf.keras.layers.Flatten(X) # this doesn't work\n",
    "    \n",
    "    # weights and biases are initialized from a normal distribution with a specified standard devation stddev\n",
    "    stddev = 0.01\n",
    "    \n",
    "    # define placeholders for weights and biases in the graph\n",
    "    weights = {\n",
    "        'hidden_layer1': tf.Variable(tf.random_normal([64*64, n_hidden_1], stddev=stddev)), # images are 64x64\n",
    "        'hidden_layer2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, 2], stddev=stddev)) \n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bias_layer1': tf.Variable(tf.random_normal([n_hidden_1], stddev=stddev)),\n",
    "        'bias_layer2': tf.Variable(tf.random_normal([n_hidden_2], stddev=stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "    }\n",
    "    \n",
    "    return weights, biases, X, Y, images_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade139c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron_old():\n",
    "        \n",
    "    weights, biases, X, Y, images_flat = allocate_weights_and_biases()\n",
    "\n",
    "    # Hidden fully connected layer 1\n",
    "    layer_1 = tf.add(tf.matmul(images_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
    "    layer_1 = tf.sigmoid(layer_1)\n",
    "\n",
    "    # Hidden fully connected layer 2\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
    "    layer_2 = tf.sigmoid(layer_2)\n",
    "    \n",
    "    # Output fully connected layer\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ba28e",
   "metadata": {},
   "source": [
    "# Define an MLP with 0-4 hidden layers, input number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4447872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def allocate_weights_and_biases(neurons_hidden):\n",
    "    \n",
    "    # define number of hidden layers ..\n",
    "    num_layers = len(neurons_hidden)     \n",
    "\n",
    "    # inputs placeholders\n",
    "    X = tf.placeholder(\"float\", [None, 64,64]) # images are 64x64\n",
    "    Y = tf.placeholder(\"float\", [None, 2])  # 2 output classes\n",
    "    \n",
    "    # flatten image features into one vector (i.e. reshape image feature matrix into a vector)\n",
    "    images_flat = tf.layers.flatten(X)  \n",
    "    #images_flat = tf.keras.layers.Flatten(X) # this doesn't work\n",
    "    \n",
    "    # weights and biases are initialized from a normal distribution with a specified standard devation stddev\n",
    "    stddev = 0.01\n",
    "    \n",
    "    # define placeholders for weights and biases in the graph\n",
    "    if num_layers==0:\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([64*64, 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "    if num_layers==1:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "        \n",
    "    if num_layers==2:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "              \n",
    "    if num_layers==3:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'hidden_layer3': tf.Variable(tf.random_normal([neurons_hidden[1], neurons_hidden[2]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'bias_layer3': tf.Variable(tf.random_normal([neurons_hidden[2]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "           \n",
    "    if num_layers==4:\n",
    "        weights = {\n",
    "            'hidden_layer1': tf.Variable(tf.random_normal([64*64, neurons_hidden[0]], stddev=stddev)),\n",
    "            'hidden_layer2': tf.Variable(tf.random_normal([neurons_hidden[0], neurons_hidden[1]], stddev=stddev)),\n",
    "            'hidden_layer3': tf.Variable(tf.random_normal([neurons_hidden[1], neurons_hidden[2]], stddev=stddev)),\n",
    "            'hidden_layer4': tf.Variable(tf.random_normal([neurons_hidden[2], neurons_hidden[3]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([neurons_hidden[num_layers-1], 2], stddev=stddev))\n",
    "        }\n",
    "        biases = {\n",
    "            'bias_layer1': tf.Variable(tf.random_normal([neurons_hidden[0]], stddev=stddev)),\n",
    "            'bias_layer2': tf.Variable(tf.random_normal([neurons_hidden[1]], stddev=stddev)),\n",
    "            'bias_layer3': tf.Variable(tf.random_normal([neurons_hidden[2]], stddev=stddev)),\n",
    "            'bias_layer4': tf.Variable(tf.random_normal([neurons_hidden[3]], stddev=stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "        }\n",
    "    \n",
    "    return weights, biases, X, Y, images_flat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3381dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(neurons_hidden):\n",
    "        \n",
    "    num_layers = len(neurons_hidden) \n",
    "    weights, biases, X, Y, images_flat = allocate_weights_and_biases(neurons_hidden=neurons_hidden)\n",
    "\n",
    "    # Hidden fully connected layer 1\n",
    "    layer_1 = tf.add(tf.matmul(images_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
    "    layer_1 = tf.sigmoid(layer_1)\n",
    "    layer_final = layer_1\n",
    "\n",
    "    # Hidden fully connected layer 2\n",
    "    if (num_layers>=2):\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
    "        layer_2 = tf.sigmoid(layer_2)\n",
    "        layer_final = layer_2\n",
    "\n",
    "    # Hidden fully connected layer 3\n",
    "    if (num_layers>=3):\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['hidden_layer3']), biases['bias_layer3'])\n",
    "        layer_3 = tf.sigmoid(layer_3)\n",
    "        layer_final = layer_3\n",
    "\n",
    "    # Hidden fully connected layer 4\n",
    "    if (num_layers>=4):\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['hidden_layer4']), biases['bias_layer4'])\n",
    "        layer_4 = tf.sigmoid(layer_4)\n",
    "        layer_final = layer_4                     \n",
    "    \n",
    "    # Output fully connected layer\n",
    "    out_layer = tf.matmul(layer_final, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9232e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2029630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  layers\n",
      "[2048, 2048, 2048, 2048]  neurons\n",
      "WARNING:tensorflow:From <ipython-input-5-696498aabe95>:11: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\Python3pt6\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-ed10c00828b2>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning parameters\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "#neurons_hidden = [512, 128, 32, 8]\n",
    "#neurons_hidden = [2048,512, 64, 8]\n",
    "#neurons_hidden = [2048,2048,256,32]\n",
    "neurons_hidden = [2048,2048,2048,2048]\n",
    "#neurons_hidden = [16,4]\n",
    "#neurons_hidden = [4096]\n",
    "#neurons_hidden = [16*1024]\n",
    "num_layers = len(neurons_hidden) # default\n",
    "num_layers = 4  # override\n",
    "neurons_hidden = neurons_hidden[:num_layers]\n",
    "print(num_layers,' layers')\n",
    "print(neurons_hidden,' neurons')\n",
    "\n",
    "# display training accuracy every ..\n",
    "display_accuracy_step = 10\n",
    "\n",
    "# set size of training and test sets\n",
    "#trainingSize = 2000\n",
    "#testSize=1000\n",
    "\n",
    "#training_images, training_labels, test_images, test_labels = get_data()\n",
    "#training_images = allImages[:trainingSize]\n",
    "#print(training_images.shape)\n",
    "#test_images = allImages[trainingSize:trainingSize+testSize]\n",
    "#print(test_images.shape)\n",
    "#trainingClasses = tumorClasses[:trainingSize]\n",
    "#print(trainingClasses.T)\n",
    "#testClasses = tumorClasses[trainingSize:trainingSize+testSize]\n",
    "#print(testClasses.T)\n",
    "logits, X, Y = multilayer_perceptron(neurons_hidden=neurons_hidden)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# define training graph operation\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# graph operation to initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa172aeb",
   "metadata": {},
   "source": [
    "# Run the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0424431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Optimization Finished!\n",
      "1  Test Accuracy: 0.925000011920929\n",
      "[[9.9902427e-01 9.9884307e-01 9.9130589e-01 9.9855584e-01 9.9856335e-01\n",
      "  9.9897087e-01 9.9893039e-01 9.9876702e-01 9.7529560e-01 5.9051746e-01\n",
      "  4.9829662e-01 8.2141882e-01 9.9893433e-01 6.9798172e-01 9.9692160e-01\n",
      "  9.9877018e-01 9.8575181e-01 9.9859256e-01 4.4516394e-01 9.9352503e-01\n",
      "  9.9904448e-01 9.9883634e-01 9.9864918e-01 9.9893004e-01 9.9868566e-01\n",
      "  9.9873096e-01 1.3977438e-01 9.9854517e-01 8.8259988e-02 9.9816114e-01\n",
      "  5.1205568e-02 7.2560835e-01 9.9861050e-01 9.9865025e-01 9.6658695e-01\n",
      "  9.0957022e-01 9.9834657e-01 9.9485153e-01 9.9878234e-01 9.9856323e-01\n",
      "  5.3216420e-02 1.7828327e-02 8.0593240e-01 9.7015184e-01 9.9818307e-01\n",
      "  9.9881905e-01 9.9800986e-01 9.9410808e-01 9.8682982e-01 5.0282203e-02\n",
      "  3.2082316e-02 9.9868363e-01 9.9900275e-01 9.9788767e-01 9.9818569e-01\n",
      "  9.9901116e-01 9.9834204e-01 9.9875212e-01 9.9856025e-01 9.9707174e-01\n",
      "  9.9896526e-01 9.9880731e-01 9.9844474e-01 9.9773586e-01 9.9889475e-01\n",
      "  9.8710710e-01 9.9704200e-01 9.9734592e-01 9.9900550e-01 9.9578506e-01\n",
      "  9.9756467e-01 9.9855584e-01 9.9894100e-01 9.9358219e-01 9.9880719e-01\n",
      "  9.9612314e-01 9.9684238e-01 9.9439538e-01 9.9845815e-01 9.9864143e-01\n",
      "  1.6266042e-02 9.9629551e-01 9.9882394e-01 9.9895006e-01 2.7662066e-01\n",
      "  9.9868912e-01 9.9799806e-01 9.9849272e-01 9.9898416e-01 9.9896455e-01\n",
      "  9.9849916e-01 9.9853075e-01 9.9818915e-01 9.9609804e-01 9.9773788e-01\n",
      "  9.9797016e-01 9.9896812e-01 1.8501557e-02 9.9785477e-01 9.9873215e-01\n",
      "  1.9598672e-02 9.5994347e-01 3.6699396e-02 9.9533647e-01 9.9889636e-01\n",
      "  1.9598672e-02 9.9614954e-01 5.1942009e-02 9.6069521e-01 9.9852520e-01\n",
      "  9.9817669e-01 9.9841845e-01 9.9896097e-01 9.9893039e-01 8.1577581e-01\n",
      "  9.9878556e-01 9.9825746e-01 9.9637932e-01 9.9879897e-01 4.8173688e-02\n",
      "  9.9891007e-01 9.9897075e-01 9.9610484e-01 9.9749631e-01 9.8674881e-01\n",
      "  9.9847776e-01 9.9900287e-01 9.9861777e-01 9.9752957e-01 9.9825174e-01\n",
      "  9.9893123e-01 9.9884367e-01 9.9799478e-01 9.9820447e-01 9.9874085e-01\n",
      "  9.9871111e-01 9.9060142e-01 8.1577581e-01 1.8496591e-01 9.8916644e-01\n",
      "  9.9897361e-01 9.9866283e-01 9.9902916e-01 7.3019856e-01 3.0077215e-02\n",
      "  9.9484730e-01 9.9638522e-01 9.9875784e-01 9.9897897e-01 9.9878460e-01\n",
      "  9.9884135e-01 1.8628420e-02 9.9626946e-01 1.9304989e-02 9.8413682e-01\n",
      "  9.9882847e-01 9.9878949e-01 9.9770504e-01 2.1547711e-02 9.9882907e-01\n",
      "  9.9890900e-01 9.9663490e-01 9.9835652e-01 9.9887866e-01 9.9880660e-01\n",
      "  9.9749446e-01 9.9779356e-01 9.9869341e-01 9.9770409e-01 9.9850792e-01\n",
      "  9.8556221e-01 9.9865556e-01 9.9230063e-01 9.9594033e-01 9.9902689e-01\n",
      "  9.9873465e-01 9.9815935e-01 9.9906224e-01 9.8803651e-01 9.9825603e-01\n",
      "  9.9864024e-01 9.9724555e-01 9.9859959e-01 9.8070908e-01 9.9340409e-01\n",
      "  9.9878019e-01 9.9858212e-01 9.9849546e-01 9.9714667e-01 9.9655849e-01\n",
      "  9.9568814e-01 9.9831545e-01 9.9306691e-01 9.9791723e-01 8.8135213e-01\n",
      "  9.9853563e-01 4.8712667e-02 9.9878520e-01 3.4088504e-02 9.9625212e-01]\n",
      " [9.7572547e-04 1.1568773e-03 8.6940527e-03 1.4441031e-03 1.4366611e-03\n",
      "  1.0291291e-03 1.0696271e-03 1.2329826e-03 2.4704468e-02 4.0948248e-01\n",
      "  5.0170338e-01 1.7858116e-01 1.0657259e-03 3.0201831e-01 3.0784283e-03\n",
      "  1.2297413e-03 1.4248126e-02 1.4074509e-03 5.5483603e-01 6.4749978e-03\n",
      "  9.5551251e-04 1.1636318e-03 1.3507882e-03 1.0699093e-03 1.3143227e-03\n",
      "  1.2690901e-03 8.6022562e-01 1.4548238e-03 9.1174001e-01 1.8388456e-03\n",
      "  9.4879448e-01 2.7439162e-01 1.3895353e-03 1.3497388e-03 3.3413079e-02\n",
      "  9.0429813e-02 1.6534374e-03 5.1485160e-03 1.2176543e-03 1.4367417e-03\n",
      "  9.4678360e-01 9.8217160e-01 1.9406758e-01 2.9848130e-02 1.8169172e-03\n",
      "  1.1808848e-03 1.9902210e-03 5.8918861e-03 1.3170152e-02 9.4971788e-01\n",
      "  9.6791774e-01 1.3163685e-03 9.9726033e-04 2.1123630e-03 1.8143031e-03\n",
      "  9.8880078e-04 1.6579032e-03 1.2478979e-03 1.4397342e-03 2.9281946e-03\n",
      "  1.0347837e-03 1.1927339e-03 1.5553213e-03 2.2640785e-03 1.1053139e-03\n",
      "  1.2892930e-02 2.9579927e-03 2.6540463e-03 9.9450198e-04 4.2149052e-03\n",
      "  2.4353571e-03 1.4441031e-03 1.0589911e-03 6.4177345e-03 1.1927821e-03\n",
      "  3.8768470e-03 3.1576606e-03 5.6046606e-03 1.5418394e-03 1.3585523e-03\n",
      "  9.8373389e-01 3.7044412e-03 1.1759894e-03 1.0499511e-03 7.2337937e-01\n",
      "  1.3108735e-03 2.0019561e-03 1.5073072e-03 1.0158516e-03 1.0354539e-03\n",
      "  1.5007984e-03 1.4692197e-03 1.8108194e-03 3.9018940e-03 2.2621450e-03\n",
      "  2.0298674e-03 1.0318889e-03 9.8149842e-01 2.1451702e-03 1.2677984e-03\n",
      "  9.8040134e-01 4.0056583e-02 9.6330065e-01 4.6635773e-03 1.1036505e-03\n",
      "  9.8040134e-01 3.8504349e-03 9.4805801e-01 3.9304763e-02 1.4748556e-03\n",
      "  1.8233428e-03 1.5815649e-03 1.0389764e-03 1.0695558e-03 1.8422416e-01\n",
      "  1.2144650e-03 1.7425440e-03 3.6206911e-03 1.2010067e-03 9.5182633e-01\n",
      "  1.0899360e-03 1.0292322e-03 3.8951391e-03 2.5036919e-03 1.3251172e-02\n",
      "  1.5223050e-03 9.9708827e-04 1.3822392e-03 2.4704011e-03 1.7482501e-03\n",
      "  1.0687873e-03 1.1562912e-03 2.0052993e-03 1.7955604e-03 1.2591177e-03\n",
      "  1.2889126e-03 9.3986299e-03 1.8422416e-01 8.1503409e-01 1.0833532e-02\n",
      "  1.0264502e-03 1.3371225e-03 9.7085431e-04 2.6980141e-01 9.6992278e-01\n",
      "  5.1527205e-03 3.6147300e-03 1.2421616e-03 1.0211092e-03 1.2154377e-03\n",
      "  1.1587297e-03 9.8137158e-01 3.7305506e-03 9.8069501e-01 1.5863141e-02\n",
      "  1.1715195e-03 1.2105493e-03 2.2950210e-03 9.7845227e-01 1.1709175e-03\n",
      "  1.0910394e-03 3.3651409e-03 1.6434891e-03 1.1213501e-03 1.1934522e-03\n",
      "  2.5055392e-03 2.2063996e-03 1.3065593e-03 2.2959495e-03 1.4921097e-03\n",
      "  1.4437731e-02 1.3444481e-03 7.6993546e-03 4.0596984e-03 9.7307214e-04\n",
      "  1.2653918e-03 1.8406090e-03 9.3782612e-04 1.1963523e-02 1.7439838e-03\n",
      "  1.3597367e-03 2.7544913e-03 1.4004243e-03 1.9290894e-02 6.5958919e-03\n",
      "  1.2198660e-03 1.4178337e-03 1.5045669e-03 2.8533726e-03 3.4415415e-03\n",
      "  4.3118950e-03 1.6845239e-03 6.9330349e-03 2.0827944e-03 1.1864792e-01\n",
      "  1.4643364e-03 9.5128739e-01 1.2148005e-03 9.6591151e-01 3.7479051e-03]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "2  Optimization Finished!\n",
      "2  Test Accuracy: 0.9100000262260437\n",
      "[[0.992019   0.9912608  0.96530306 0.9884332  0.98969346 0.9908309\n",
      "  0.99115974 0.9900591  0.8074931  0.49486047 0.39301836 0.6753496\n",
      "  0.9911088  0.39952707 0.98200274 0.990546   0.87003005 0.98999596\n",
      "  0.46066666 0.904154   0.9922178  0.99100375 0.98957705 0.9915029\n",
      "  0.9889337  0.99035174 0.30393702 0.9897794  0.25325936 0.9878549\n",
      "  0.19370268 0.43065837 0.9902568  0.9898341  0.92348206 0.66218245\n",
      "  0.9894371  0.9561572  0.99075377 0.98863375 0.24053763 0.14554948\n",
      "  0.53797245 0.8329769  0.98888844 0.99108267 0.9858014  0.94427145\n",
      "  0.90886235 0.23339021 0.19276595 0.9902304  0.9917379  0.98672915\n",
      "  0.9863906  0.9921619  0.9890316  0.9906611  0.98845726 0.98276985\n",
      "  0.99159664 0.99075425 0.98952484 0.9858645  0.9913394  0.9706014\n",
      "  0.9727232  0.9762798  0.99209213 0.9786542  0.98511106 0.9884332\n",
      "  0.9916745  0.9729355  0.9905653  0.98106784 0.97909284 0.967951\n",
      "  0.98873085 0.9899037  0.14676452 0.9789932  0.98951554 0.9917331\n",
      "  0.33152932 0.9889649  0.9872436  0.98822486 0.99177164 0.991861\n",
      "  0.98973453 0.9898662  0.9878802  0.9518518  0.9858306  0.9865238\n",
      "  0.99216545 0.14341572 0.98711675 0.9899719  0.1623127  0.87568897\n",
      "  0.199667   0.9763099  0.99119323 0.1623127  0.9742958  0.18489198\n",
      "  0.8094728  0.98874503 0.9891348  0.9885378  0.9916824  0.9914238\n",
      "  0.72097766 0.99093515 0.9875143  0.9813705  0.99043876 0.24600892\n",
      "  0.9915575  0.9917698  0.9737794  0.9829943  0.87224483 0.9892223\n",
      "  0.9921163  0.9896768  0.98125917 0.9874728  0.9912747  0.9911247\n",
      "  0.9852571  0.98608243 0.9910642  0.9891973  0.9618749  0.72097766\n",
      "  0.2596694  0.95905876 0.99129206 0.98730993 0.9920598  0.6543729\n",
      "  0.17253686 0.95269704 0.98224306 0.9906598  0.9917854  0.99110657\n",
      "  0.99070495 0.1510476  0.97713155 0.15330978 0.9377451  0.9910252\n",
      "  0.99080116 0.9728889  0.15829104 0.99092406 0.9916516  0.9750899\n",
      "  0.98708296 0.9911889  0.9909114  0.98314416 0.9868832  0.9897451\n",
      "  0.98187584 0.9891405  0.9134214  0.99025273 0.9678937  0.97609234\n",
      "  0.99212015 0.99066716 0.9883378  0.99232846 0.9513206  0.9886906\n",
      "  0.9897509  0.96743315 0.9901001  0.9336524  0.95273167 0.99046046\n",
      "  0.98767984 0.99032366 0.98511434 0.9825089  0.9733565  0.9889731\n",
      "  0.94837666 0.9872405  0.7678916  0.98903733 0.20400973 0.99042284\n",
      "  0.19556136 0.98328227]\n",
      " [0.00798096 0.00873923 0.03469696 0.0115668  0.0103065  0.00916909\n",
      "  0.00884027 0.00994097 0.19250692 0.5051395  0.60698164 0.32465044\n",
      "  0.00889122 0.6004729  0.01799733 0.00945398 0.12996994 0.0100041\n",
      "  0.5393333  0.095846   0.00778219 0.00899628 0.01042301 0.00849718\n",
      "  0.01106637 0.00964825 0.6960629  0.0102206  0.74674064 0.01214505\n",
      "  0.8062973  0.56934166 0.00974321 0.01016599 0.07651795 0.33781755\n",
      "  0.01056285 0.04384279 0.00924627 0.01136621 0.75946236 0.8544505\n",
      "  0.4620275  0.16702309 0.01111155 0.00891729 0.01419864 0.05572858\n",
      "  0.0911376  0.76660985 0.807234   0.00976961 0.00826205 0.01327079\n",
      "  0.01360945 0.0078381  0.01096836 0.00933886 0.01154268 0.01723018\n",
      "  0.00840337 0.00924573 0.0104751  0.01413549 0.00866064 0.02939858\n",
      "  0.02727683 0.0237202  0.0079079  0.02134576 0.01488891 0.0115668\n",
      "  0.00832549 0.02706452 0.00943469 0.01893209 0.02090716 0.03204904\n",
      "  0.01126917 0.01009628 0.85323554 0.0210068  0.01048443 0.00826697\n",
      "  0.6684707  0.0110351  0.01275633 0.01177518 0.00822836 0.008139\n",
      "  0.01026546 0.01013382 0.01211988 0.04814822 0.01416945 0.0134762\n",
      "  0.00783458 0.8565843  0.0128832  0.01002808 0.8376873  0.12431105\n",
      "  0.800333   0.0236901  0.00880681 0.8376873  0.02570423 0.81510806\n",
      "  0.19052717 0.01125489 0.01086521 0.01146226 0.00831761 0.00857629\n",
      "  0.2790224  0.00906482 0.0124857  0.01862948 0.00956124 0.7539911\n",
      "  0.00844248 0.0082302  0.02622065 0.01700573 0.12775517 0.01077772\n",
      "  0.00788378 0.01032321 0.01874084 0.01252725 0.00872524 0.00887526\n",
      "  0.01474295 0.01391759 0.00893577 0.01080261 0.03812514 0.2790224\n",
      "  0.7403306  0.04094119 0.00870795 0.01268999 0.00794014 0.34562713\n",
      "  0.82746315 0.04730301 0.01775699 0.00934024 0.00821463 0.00889347\n",
      "  0.00929501 0.84895235 0.02286847 0.84669024 0.06225498 0.00897479\n",
      "  0.00919891 0.02711107 0.84170896 0.0090759  0.00834841 0.02491005\n",
      "  0.01291711 0.00881117 0.00908855 0.01685576 0.01311674 0.010255\n",
      "  0.0181242  0.01085947 0.08657858 0.00974728 0.03210625 0.02390768\n",
      "  0.0078798  0.00933282 0.01166215 0.00767159 0.04867939 0.01130932\n",
      "  0.01024908 0.03256689 0.00989993 0.06634757 0.04726832 0.00953955\n",
      "  0.0123201  0.00967632 0.01488571 0.01749111 0.02664358 0.01102696\n",
      "  0.05162336 0.01275952 0.23210843 0.01096269 0.7959903  0.00957712\n",
      "  0.80443865 0.0167178 ]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "Mean Test Accuracy =  0.9175000190734863\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    num_sessions = 2\n",
    "    accuracy_eval = numpy.zeros(num_sessions)\n",
    "    \n",
    "    for session in range(num_sessions):\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init_op)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # complete code below\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            #_, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
    "                                                               #Y: trainingClasses})\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: trainingImages,\n",
    "                                                               Y: trainingTumorClasses})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            #print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                #print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: trainingClasses})))\n",
    "\n",
    "        print(session+1,\" Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            \n",
    "        # run test accuracy operation ..\n",
    "        accuracy_eval[session] = accuracy.eval({X: testImages, Y: testTumorClasses})\n",
    "        print(session+1,\" Test Accuracy:\", accuracy_eval[session])\n",
    "        #print(\"Test Accuracy:\", accuracy.eval({X: test_images, Y: testClasses}))\n",
    "        \n",
    "        predictions = pred.eval(feed_dict = {X:testImages})\n",
    "        print(predictions.T)\n",
    "        predictions_onehot = predictions.copy()\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            #print(i,'   ',prediction)\n",
    "            if prediction[0] > prediction[1]:\n",
    "                predictions_onehot[i] = [1,0]\n",
    "            else:\n",
    "                predictions_onehot[i] = [0,1]\n",
    "        print(predictions_onehot.T)\n",
    "        \n",
    "    print('Mean Test Accuracy = ',numpy.mean(accuracy_eval))\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff85ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Actual values in X\n",
      "Predicted values in Y\n",
      "[[ 23   4]\n",
      " [ 14 159]] \n",
      "\n",
      "37  cases of no tumor:  62.2 % chance of correct prediction\n",
      "163  cases of a tumor:  97.5 % chance of correct prediction\n",
      "\n",
      "27  predictions of no tumor are correct  85.2 % of the time\n",
      "173  predictions of a tumor are correct  91.9 % of the time\n"
     ]
    }
   ],
   "source": [
    "   #print(testClasses.T)\n",
    "   #print(predictions_onehot.T)\n",
    "   print('Confusion Matrix')\n",
    "   print('Actual values in X')\n",
    "   print('Predicted values in Y')\n",
    "   confusionMatrix = confusion_matrix(predictions_onehot.T[0],testTumorClasses.T[0])\n",
    "   print(confusionMatrix,'\\n')\n",
    " \n",
    "   inputNumbers = numpy.sum(confusionMatrix, axis=0)\n",
    "   print(inputNumbers[0],' cases of no tumor: ',round(100*confusionMatrix[0,0]/inputNumbers[0],1),'% chance of correct prediction')\n",
    "   print(inputNumbers[1],' cases of a tumor: ',round(100*confusionMatrix[1,1]/inputNumbers[1],1),'% chance of correct prediction\\n') \n",
    "\n",
    "   outputNumbers = numpy.sum(confusionMatrix, axis=1)\n",
    "   print(outputNumbers[0],' predictions of no tumor are correct ',round(100*confusionMatrix[0,0]/outputNumbers[0],1),'% of the time')\n",
    "   print(outputNumbers[1],' predictions of a tumor are correct ',round(100*confusionMatrix[1,1]/outputNumbers[1],1),'% of the time')\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f03e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
